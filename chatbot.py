# -*- coding: utf-8 -*-
"""chatbot

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jHupKCPnA3HhdVEVatzSEJThXWjGEQ7L

Building a Transformer based language model
A character level language model
Meaning a model that predicts next character to answer the questions eg. Chatgpt by OpenAI
Resources:
- Attention is All You Need paper: https://arxiv.org/abs/1706.03762
- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165
- OpenAI ChatGPT blog post: https://openai.com/blog/chatgpt/
Muralidhar Bhusal
2023-07-20
"""

import pandas as pd
import numpy as np
import torch
from torch._C import dtype
import torch
import torch.nn as nn
from torch.nn import functional as F

# Hyperparameters
batch_size = 32
block_size = 8
max_iters = 3200
eval_interval = 300
learning_rate = 1e-2
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 200
# <==========================================>

torch.manual_seed(1250)

# <==========================================>

# import and read data
data = 'Data _ A History of Nepal.txt'
with open(data,'r') as fp:
  data_raw = fp.read()

print("The length of data is", len(data_raw))

# Cleaning the data
data_new = " ".join([i.strip('\n') for i in data_raw.split()]) 
# print(data_new)

# list of all the characters that occur in the data
characters = sorted(list(set(data_new)))
vocab_size = len(characters)
print("".join(characters))
print(len(characters))

#<================================================>

"""Creating a simple encode and decode function to tokenize the characters."""
# Creating tokenizers
stoi = {ch:i for i, ch in enumerate (characters)} # Maping
itos = {i:ch for i, ch in enumerate (characters)} # Reverse Mapping
# Encoding each characters
encode = lambda s: [stoi[c] for c in s] # Takes a string and outputs a list of integers
# Decoding each character i.e reverse mapping
decode = lambda l: "".join([itos[i] for i in l]) # Takes a list of integers outputs a string
# print(encode("hi there"))
# print(decode(encode("hi there")))

# TODO
# There are other encoders or tokenizers like tiktoken by OpenAI and Sentencepiece by google

# <======================================================>

"""Encoding the entire dataset and storing it in a torch tensor. I will be using pytorch for this project. The data encoded will be stores in torch.tensor"""
data = torch.tensor(encode(data_new), dtype = torch.long)
# print(data.shape, data.dtype)
# print(data[:1000]) # For the computer

"""Train Test Split"""

n = int(0.9*len(data)) # Use 90% of the data as training data
train_data = data[:n]
val_data = data[n:] # Because Andrej Karpathy said so

"""Plugging and feeding the data to the transformers.
feeding the whole data can be heavy for computation.
Feed small chunks of data to the transformers.
"""
'''
block_size = 8
print(train_data[:block_size+1]) # Why +1??
x =train_data[:block_size+1]
y = train_data[1:block_size+1]
for tensors in range (block_size):
  context = x[:tensors+1]
  target = y[tensors]
  print(f"When input is {context} output is {target}")

torch.manual_seed(1250)
batch_size = 4 # No of independent sequence to be trained in parallel
block_size = 8 # Maximum context length for prediction
'''
def get_batch(split):
  #generate a small batch of data of inputs x and targets y
  data = train_data if split == "train" else val_data
  ix = torch.randint(len(data) - block_size, (batch_size,))
  x = torch.stack([data[i:i+block_size] for i in ix])
  y = torch.stack([data[i+1:i+block_size+1] for i in ix])
  x,y = x.to(device), y.to(device)
  return x,y

@torch.no_grad()
def estimate_loss():
  out = {}
  model.eval()
  for split in['train','val']:
    losses = torch.zeros(eval_iters)
    for k in range(eval_iters):
      X,Y = get_batch(split)
      logits , loss = model(X,Y)
      losses[k]= loss.item()
    out[split] = losses.mean()
  model.train()
  return out

# xb, yb = get_batch('train')
# print("inputs: ")
# print(xb.shape)
# print(xb)
# print("targets: ")
# print(yb.shape)
# print(yb)

# print("---------------------------------------------------------------------")

# for b in range(batch_size):
#   for t in range(block_size):
#     context = xb[b, :t+1]
#     target = yb[b, t]
#     print(f"When the context is {context.tolist()} output is {target}")

# from torch._C import dtype
# import torch
# import torch.nn as nn
# from torch.nn import functional as F
# torch.manual_seed(1337)



# A simple Bigram model
class BigramLM(nn.Module):

  def __init__ (self,vocab_size):
    super().__init__()
    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)


  def forward(self, idx, targets=None):

    logits = self.token_embedding_table(idx)

    if targets is None:
      loss = None

    else:
      B, T, C = logits.shape
      logits = logits.view(B*T, C)
      targets = targets.view(B*T)
      loss = F.cross_entropy(logits, targets)

    return logits, loss

  def generate(self, idx, max_new_tokens):
    for _ in range(max_new_tokens):

      logits, loss= self(idx)

      logits = logits[:, -1, :]

      # Apply Softmax to get the probablities

      probs= F.softmax(logits, dim=-1)

      idx_next = torch.multinomial(probs, num_samples=1)

      idx= torch.cat((idx, idx_next), dim=1)

    return idx







model = BigramLM(vocab_size)
m = model.to(device)
# logits, loss = m(xb, yb)
# print(logits.shape)
# print(loss)
# print(decode(m.generate(idx = torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))


# Creating a pythorch optimizer
optimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)

for iter in range(max_iters):

  if iter % eval_interval == 0:
    losses = estimate_loss()
    print(f"Step{iter}: Train Loss {losses['train']:.4f}, val loss {losses['val']:.4f}")


  xb, yb =get_batch('train')

  # Evaluate the loss

  logits, loss =m(xb,yb)
  optimizer.zero_grad(set_to_none=True)
  loss.backward()
  optimizer.step()

# Generate from the model
context =  torch.zeros((1, 1), dtype = torch.long, device = device)

print(decode(m.generate(context, max_new_tokens = 500)[0].tolist()))

